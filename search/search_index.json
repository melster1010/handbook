{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the DANDI archive documentation \u00b6 The Web interface to the DANDI archive is located at https://dandiarchive.org . This site provides information on how to interact with the archive. Where to communicate: \u00b6 You can ask questions, report bugs, or request features at our helpdesk For interacting with the global neuroscience community post on https://neurostars.org and use the tag dandi . To upload data or to use the DANDI Jupyterhub please register on DANDI using Github . After approving your registration, we will invite you to the DANDI Slack workspace. See here for details on how to register . Email us: info@dandiarchive.org How to use this documentation \u00b6 To start using the archive head over to: Working with DANDI If you want to know more about the DANDI project, its goals and the problems it tries to solve, check out: our introduction . The DANDI archive stores neurophysiology datasets including electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments using NWB , BIDS , and other BRAIN Initiative standards. Not sure how the project is organized? Check out the project structure page. Licence \u00b6 This work is licensed under a Creative Commons Attribution 4.0 International License . Contributing and feedback \u00b6 We are looking for people to give us feedback on this documentation if anything is unclear by opening an issue on our repository . You can also get in touch on our Slack channel. You will be invited once you [register an account] on the archive. If you want to get started right away and contribute directly to this documentation, you can find references and how-to in the about section .","title":"Welcome"},{"location":"#welcome-to-the-dandi-archive-documentation","text":"The Web interface to the DANDI archive is located at https://dandiarchive.org . This site provides information on how to interact with the archive.","title":"Welcome to the DANDI archive documentation"},{"location":"#where-to-communicate","text":"You can ask questions, report bugs, or request features at our helpdesk For interacting with the global neuroscience community post on https://neurostars.org and use the tag dandi . To upload data or to use the DANDI Jupyterhub please register on DANDI using Github . After approving your registration, we will invite you to the DANDI Slack workspace. See here for details on how to register . Email us: info@dandiarchive.org","title":"Where to communicate:"},{"location":"#how-to-use-this-documentation","text":"To start using the archive head over to: Working with DANDI If you want to know more about the DANDI project, its goals and the problems it tries to solve, check out: our introduction . The DANDI archive stores neurophysiology datasets including electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments using NWB , BIDS , and other BRAIN Initiative standards. Not sure how the project is organized? Check out the project structure page.","title":"How to use this documentation"},{"location":"#licence","text":"This work is licensed under a Creative Commons Attribution 4.0 International License .","title":"Licence"},{"location":"#contributing-and-feedback","text":"We are looking for people to give us feedback on this documentation if anything is unclear by opening an issue on our repository . You can also get in touch on our Slack channel. You will be invited once you [register an account] on the archive. If you want to get started right away and contribute directly to this documentation, you can find references and how-to in the about section .","title":"Contributing and feedback"},{"location":"01_introduction/","text":"Introduction \u00b6 Advantages of using DANDI \u00b6 An open data archive to submit neurophysiology data for electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments A persistent, versioned and growing collection of standardized datasets. Rich metadata to support search across data. A place to house data to collaborate across research sites. Consistent and transparent data standards to simplify software development. Supported by the BRAIN Initiative and the AWS Public dataset programs. The challenges \u00b6 To know which data are useful, data has to be accessible. Non standardized datasets lead to significant resources needed to understand and adapt code to these datasets. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. There are many domain general places to house data (e.g., Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant datasets. Datasets are growing larger requiring compute services to be closer to data. Neurotechnology is evolving and requires flexible extensions to metadata and data storage requirements. Consolidating and creating robust algorithms (e.g., spike sorting) requires varied data sources. Our solution \u00b6 We have developed a FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized neurophysiology and associated data. We use the Neurodata Without Borders , Brain Imaging Data Structure , Neuroimaging Data Model and other BRAIN Initiative standards to organize and search the data. A Jupyterhub-based analysis platform provides easy access to the data. The data can be accessed programmatically allowing for new software and tools to be built. The archive itself is built on a software stack of open source products, thus enriching the ecosystem. The archive provides persistent identifiers for versioned datasets thus improving reproducibility of neurophysiology research.","title":"Introduction"},{"location":"01_introduction/#introduction","text":"","title":"Introduction"},{"location":"01_introduction/#advantages-of-using-dandi","text":"An open data archive to submit neurophysiology data for electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments A persistent, versioned and growing collection of standardized datasets. Rich metadata to support search across data. A place to house data to collaborate across research sites. Consistent and transparent data standards to simplify software development. Supported by the BRAIN Initiative and the AWS Public dataset programs.","title":"Advantages of using DANDI"},{"location":"01_introduction/#the-challenges","text":"To know which data are useful, data has to be accessible. Non standardized datasets lead to significant resources needed to understand and adapt code to these datasets. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. There are many domain general places to house data (e.g., Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant datasets. Datasets are growing larger requiring compute services to be closer to data. Neurotechnology is evolving and requires flexible extensions to metadata and data storage requirements. Consolidating and creating robust algorithms (e.g., spike sorting) requires varied data sources.","title":"The challenges"},{"location":"01_introduction/#our-solution","text":"We have developed a FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized neurophysiology and associated data. We use the Neurodata Without Borders , Brain Imaging Data Structure , Neuroimaging Data Model and other BRAIN Initiative standards to organize and search the data. A Jupyterhub-based analysis platform provides easy access to the data. The data can be accessed programmatically allowing for new software and tools to be built. The archive itself is built on a software stack of open source products, thus enriching the ecosystem. The archive provides persistent identifiers for versioned datasets thus improving reproducibility of neurophysiology research.","title":"Our solution"},{"location":"100_about_this_doc/","text":"About this documentation \u00b6 This documentation is a work in progress and we welcome all input: if something is missing or unclear, let us know by opening an issue on our helpdesk . Serving the docs locally \u00b6 This project uses the MkDocs tool with the Material theme and extra plugins to generate the website. To test locally, you will need to install the Python dependencies. To do that, type the following commands: git clone https://github.com/dandi/handbook.git cd handbook pip install -r requirements.txt If you are working on your fork , simply replace https://github.com/dandi/handbook.git by git clone git@github.com/<username>/handbook.git where <username> is your GitHub username Once done, you need to run MkDocs. Simply type: mkdocs serve Finally, open up http://127.0.0.1:8000/ in your browser, and you should see the default home page of the documentation being displayed.","title":"About this doc"},{"location":"100_about_this_doc/#about-this-documentation","text":"This documentation is a work in progress and we welcome all input: if something is missing or unclear, let us know by opening an issue on our helpdesk .","title":"About this documentation"},{"location":"100_about_this_doc/#serving-the-docs-locally","text":"This project uses the MkDocs tool with the Material theme and extra plugins to generate the website. To test locally, you will need to install the Python dependencies. To do that, type the following commands: git clone https://github.com/dandi/handbook.git cd handbook pip install -r requirements.txt If you are working on your fork , simply replace https://github.com/dandi/handbook.git by git clone git@github.com/<username>/handbook.git where <username> is your GitHub username Once done, you need to run MkDocs. Simply type: mkdocs serve Finally, open up http://127.0.0.1:8000/ in your browser, and you should see the default home page of the documentation being displayed.","title":"Serving the docs locally"},{"location":"10_using_dandi/","text":"Working with DANDI \u00b6 DANDI provides access to an archive that stores cellular neurophysiology datasets. We refer to such datasets as Dandisets . A Dandiset is organized in a structured manner to help users and software tools interact with it. Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g., https://identifiers.org/DANDI:000004 ). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset . DANDI components \u00b6 The DANDI Web application \u00b6 The DANDI Web application allows you to: Browse Dandisets . Search across Dandisets . Create an account to register a new Dandiset or gain access to the Dandihub analysis platform . Add collaborators to your Dandiset . Retrieve an API key to perform data upload to your Dandisets . Publish versions of your Dandisets . If you want to learn more, go to Working with DANDI Archive web The DANDI Python client \u00b6 The DANDI Python client allows you to: Download Danidsets and individual subject folders or files. Organize your data locally before upload. Upload Dandisets . The Dandihub analysis platform \u00b6 Dandihub provides a Jupyter environment to interact with the DANDI archive. To use the hub, you will need to register an account using the DANDI Web application . Please note that Dandihub is not intended for significant computation, but provides a place to introspect Dandisets and files. Downloading from DANDI \u00b6 You can download entire Dandisets or single files. Downloading a file \u00b6 Using the Web application \u00b6 Each Dandiset has a View Data option. This provides a folder-like view to navigate a Dandiset . Any file in the Dandiset has a download icon next to it. You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. You can then use this URL programmatically or in other applications such as the NWB Explorer or in a Jupyter notebook on Dandihub . Using the Python CLI \u00b6 First install the Python client using pip install dandi in a Python 3.7+ environment. Downloading a Dandiset , e.g.: dandi download DANDI:000023 Downloading data for a specific subject from a dandiset (names of the subjects could be found on the gui.dandiarchive.org website or by running dandi ls -r DANDI:000023 ), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/draft/assets/?path=sub-P10HMH or a specific version by replacing draft with it (e.g. 0.210914.1900 in the case of this dandiset) Downloading a specific file from a dandiset (a link for the specific file could be found on the gui.dandiarchive.org website), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/0.210914.1900/assets/1a93dc97-327d-4f9c-992d-c2149e7810ae/download/ Hint: dandi download supports a number of resource identifiers to point to dandiset, folder, or file. Providing an incorrect URL (e.g. dandi download wrongurl ) will provide a list of supported identifiers. Create an account on DANDI \u00b6 To create an account on DANDI, you will need to. Create a Github account if you don't have one. Using your Github account register a DANDI account . You will receive an email acknowledging activation of your account within 24 hours. You can now login to DANDI using the Github by clicking on the login button. Uploading a Dandiset \u00b6 DANDI provides a production server (http://dandiarchive.org/) for real data and an identical staging server (https://gui-staging.dandiarchive.org/) for test data. When you create a dandiset, a permanent ID is automatically assigned to it. To prevent the production server from being inundated with test dandisets, we encourage developers to develop against the staging server. The below instructions will alert you to where the commands for interacting with these two different servers differ slightly. Setup If you do not have a DANDI account, please create an account Log in to DANDI and copy your API key. Click on your user initials on the top right after logging in. Production (dandiarchive.org) and staging (gui-staging.dandiarchive.org) servers have different API keys and different logins. Locally Create a Python environment (not required, but strongly recommended, e.g., miniconda , virtualenv Install the DANDI CLI into your Python environment pip install -U dandi Store your API key somewhere that the CLI can find it; see \"Storing Access Credentials\" below. Data upload/management workflow Register a dandiset to generate an identifier. You will be asked to enter basic metadata, a name (title) and description (abstract) for your dataset. Click NEW DANDISET in the Web application (top right corner) after logging in. After you provide name and description, the dataset identifer will be created, we will call this <dataset_id> . NWB format: Convert your data to NWB 2.1+ in a local folder. Let's call this <source_folder> This step can be complex depending on your data. Feel free to reach out to us for help . Validate the NWB files by running: dandi validate <source_folder> . If you are having trouble with validation, make sure the conversions were run with the most recent version of PyNWB and MatNWB . Preparing a dataset folder for upload: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft cd <dataset_id> dandi organize <source_folder> -f dry dandi organize <source_folder> dandi upload # for staging: dandi upload -i dandi-staging Add metadata by visiting your dandiset landing page: https://dandiarchive.org/dandiset/<dataset_id>/draft and clicking on the METADATA link. Use the dandiset URL in your preprint directly, or download it using the dandi CLI: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft BIDS format: Please reach out to Dandi team for help . Storing Access Credentials \u00b6 By default, the DANDI CLI looks for an API key in the DANDI_API_KEY environment variable. To set this on linux or osx, in run export DANDI_API_KEY=personal-key-value (note that there are no spaces around the \"=\"). If this is not set, the CLI will look up the API key using the keyring library, which supports numerous backends, including the system keyring, an encrypted keyfile, and a plaintext (unencrypted) keyfile. You can store your API key where the keyring library can find it by using the keyring program: Run keyring set dandi-api-dandi key and enter the API key when asked for the password for key in dandi-api-dandi . You can set the backend the keyring library uses either by setting the PYTHON_KEYRING_BACKEND environment variable or by filling in the keyring library's configuration file . IDs for the available backends can be listed by running keyring --list . If no backend is specified in this way, the library will use the available backend with the highest priority. If the API key isn't stored in either the DANDI_API_KEY environment variable or in the keyring, the CLI will prompt you to enter the API key, and then it will store it in the keyring. This may cause you to be prompted further; you may be asked to enter a password to encrypt/decrypt the keyring, or you may be asked by your OS to confirm whether to give the DANDI CLI access to the keyring. If the DANDI CLI encounters an error while attempting to fetch the API key from the default keyring backend, it will fall back to using an encrypted keyfile (the keyrings.alt.file.EncryptedKeyring backend). If the keyfile does not already exist, the CLI will ask you for confirmation; if you answer \"yes,\" the keyring configuration file (if does not already exist; see above) will be configured to use EncryptedKeyring as the default backend. If you answer \"no,\" the CLI will exit with an error, and you must store the API key somewhere accessible to the CLI on your own. Publish a Dandiset \u00b6 \ud83d\udee0 Work in progress \ud83d\udee0 Acquiring Debugging Information \u00b6 In the event that something goes wrong while using the dandi client, the first place to check for more information so that you can file a quality bug report is the logs. Every dandi command records a copy of its logs in a logfile, the location of which is reported to the user when the command finishes running. The location of the logs varies by platform: Linux: ~/.cache/dandi-cli/log or $XDG_CACHE_HOME/dandi-cli/log macOS: ~/Library/Logs/dandi-cli Windows XP: C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\dandi\\dandi-cli\\Logs Windows Vista: C:\\Users\\<username>\\AppData\\Local\\dandi\\dandi-cli\\Logs Logs are named with a combination of the time at which the dandi command started running and the process ID of the command. Recent versions of dandi include all possible debugging information in the logs, but if you're using an older version, only log messages that were printed to the user when the command ran are recorded. As a result, in order to get complete debugging information, you may have to rerun the problematic command, this time increasing the logging level by passing -l DEBUG or --log-level DEBUG on the command line. Note that this option goes between the main dandi command and the name of the subcommand: # Right: dandi -l DEBUG upload # Wrong: dandi upload -l DEBUG In addition, many commands can be put into a developer-specific mode for showing raw progress information instead of fancy progress bars. For the delete , organize , upload , and validate commands, this can be done by setting the DANDI_DEVEL environment variable and passing --devel-debug to the command, like so: DANDI_DEVEL=1 dandi upload --devel-debug For the download command, the equivalent is the -f debug / --format debug option: dandi download -f debug More advanced users who are familiar with the Python debugger can instruct dandi to automatically open the debugger if any errors occur by supplying the --pdb option to the command. Like the -l / --log-level option, the --pdb option must be placed between dandi and the name of the subcommand.","title":"Using DANDI"},{"location":"10_using_dandi/#working-with-dandi","text":"DANDI provides access to an archive that stores cellular neurophysiology datasets. We refer to such datasets as Dandisets . A Dandiset is organized in a structured manner to help users and software tools interact with it. Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g., https://identifiers.org/DANDI:000004 ). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset .","title":"Working with DANDI"},{"location":"10_using_dandi/#dandi-components","text":"","title":"DANDI components"},{"location":"10_using_dandi/#the-dandi-web-application","text":"The DANDI Web application allows you to: Browse Dandisets . Search across Dandisets . Create an account to register a new Dandiset or gain access to the Dandihub analysis platform . Add collaborators to your Dandiset . Retrieve an API key to perform data upload to your Dandisets . Publish versions of your Dandisets . If you want to learn more, go to Working with DANDI Archive web","title":"The DANDI Web application"},{"location":"10_using_dandi/#the-dandi-python-client","text":"The DANDI Python client allows you to: Download Danidsets and individual subject folders or files. Organize your data locally before upload. Upload Dandisets .","title":"The DANDI Python client"},{"location":"10_using_dandi/#the-dandihub-analysis-platform","text":"Dandihub provides a Jupyter environment to interact with the DANDI archive. To use the hub, you will need to register an account using the DANDI Web application . Please note that Dandihub is not intended for significant computation, but provides a place to introspect Dandisets and files.","title":"The Dandihub analysis platform"},{"location":"10_using_dandi/#downloading-from-dandi","text":"You can download entire Dandisets or single files.","title":"Downloading from DANDI"},{"location":"10_using_dandi/#downloading-a-file","text":"","title":"Downloading a file"},{"location":"10_using_dandi/#using-the-web-application","text":"Each Dandiset has a View Data option. This provides a folder-like view to navigate a Dandiset . Any file in the Dandiset has a download icon next to it. You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. You can then use this URL programmatically or in other applications such as the NWB Explorer or in a Jupyter notebook on Dandihub .","title":"Using the Web application"},{"location":"10_using_dandi/#using-the-python-cli","text":"First install the Python client using pip install dandi in a Python 3.7+ environment. Downloading a Dandiset , e.g.: dandi download DANDI:000023 Downloading data for a specific subject from a dandiset (names of the subjects could be found on the gui.dandiarchive.org website or by running dandi ls -r DANDI:000023 ), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/draft/assets/?path=sub-P10HMH or a specific version by replacing draft with it (e.g. 0.210914.1900 in the case of this dandiset) Downloading a specific file from a dandiset (a link for the specific file could be found on the gui.dandiarchive.org website), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/0.210914.1900/assets/1a93dc97-327d-4f9c-992d-c2149e7810ae/download/ Hint: dandi download supports a number of resource identifiers to point to dandiset, folder, or file. Providing an incorrect URL (e.g. dandi download wrongurl ) will provide a list of supported identifiers.","title":"Using the Python CLI"},{"location":"10_using_dandi/#create-an-account-on-dandi","text":"To create an account on DANDI, you will need to. Create a Github account if you don't have one. Using your Github account register a DANDI account . You will receive an email acknowledging activation of your account within 24 hours. You can now login to DANDI using the Github by clicking on the login button.","title":"Create an account on DANDI"},{"location":"10_using_dandi/#uploading-a-dandiset","text":"DANDI provides a production server (http://dandiarchive.org/) for real data and an identical staging server (https://gui-staging.dandiarchive.org/) for test data. When you create a dandiset, a permanent ID is automatically assigned to it. To prevent the production server from being inundated with test dandisets, we encourage developers to develop against the staging server. The below instructions will alert you to where the commands for interacting with these two different servers differ slightly. Setup If you do not have a DANDI account, please create an account Log in to DANDI and copy your API key. Click on your user initials on the top right after logging in. Production (dandiarchive.org) and staging (gui-staging.dandiarchive.org) servers have different API keys and different logins. Locally Create a Python environment (not required, but strongly recommended, e.g., miniconda , virtualenv Install the DANDI CLI into your Python environment pip install -U dandi Store your API key somewhere that the CLI can find it; see \"Storing Access Credentials\" below. Data upload/management workflow Register a dandiset to generate an identifier. You will be asked to enter basic metadata, a name (title) and description (abstract) for your dataset. Click NEW DANDISET in the Web application (top right corner) after logging in. After you provide name and description, the dataset identifer will be created, we will call this <dataset_id> . NWB format: Convert your data to NWB 2.1+ in a local folder. Let's call this <source_folder> This step can be complex depending on your data. Feel free to reach out to us for help . Validate the NWB files by running: dandi validate <source_folder> . If you are having trouble with validation, make sure the conversions were run with the most recent version of PyNWB and MatNWB . Preparing a dataset folder for upload: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft cd <dataset_id> dandi organize <source_folder> -f dry dandi organize <source_folder> dandi upload # for staging: dandi upload -i dandi-staging Add metadata by visiting your dandiset landing page: https://dandiarchive.org/dandiset/<dataset_id>/draft and clicking on the METADATA link. Use the dandiset URL in your preprint directly, or download it using the dandi CLI: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft BIDS format: Please reach out to Dandi team for help .","title":"Uploading a Dandiset"},{"location":"10_using_dandi/#storing-access-credentials","text":"By default, the DANDI CLI looks for an API key in the DANDI_API_KEY environment variable. To set this on linux or osx, in run export DANDI_API_KEY=personal-key-value (note that there are no spaces around the \"=\"). If this is not set, the CLI will look up the API key using the keyring library, which supports numerous backends, including the system keyring, an encrypted keyfile, and a plaintext (unencrypted) keyfile. You can store your API key where the keyring library can find it by using the keyring program: Run keyring set dandi-api-dandi key and enter the API key when asked for the password for key in dandi-api-dandi . You can set the backend the keyring library uses either by setting the PYTHON_KEYRING_BACKEND environment variable or by filling in the keyring library's configuration file . IDs for the available backends can be listed by running keyring --list . If no backend is specified in this way, the library will use the available backend with the highest priority. If the API key isn't stored in either the DANDI_API_KEY environment variable or in the keyring, the CLI will prompt you to enter the API key, and then it will store it in the keyring. This may cause you to be prompted further; you may be asked to enter a password to encrypt/decrypt the keyring, or you may be asked by your OS to confirm whether to give the DANDI CLI access to the keyring. If the DANDI CLI encounters an error while attempting to fetch the API key from the default keyring backend, it will fall back to using an encrypted keyfile (the keyrings.alt.file.EncryptedKeyring backend). If the keyfile does not already exist, the CLI will ask you for confirmation; if you answer \"yes,\" the keyring configuration file (if does not already exist; see above) will be configured to use EncryptedKeyring as the default backend. If you answer \"no,\" the CLI will exit with an error, and you must store the API key somewhere accessible to the CLI on your own.","title":"Storing Access Credentials"},{"location":"10_using_dandi/#publish-a-dandiset","text":"\ud83d\udee0 Work in progress \ud83d\udee0","title":"Publish a Dandiset"},{"location":"10_using_dandi/#acquiring-debugging-information","text":"In the event that something goes wrong while using the dandi client, the first place to check for more information so that you can file a quality bug report is the logs. Every dandi command records a copy of its logs in a logfile, the location of which is reported to the user when the command finishes running. The location of the logs varies by platform: Linux: ~/.cache/dandi-cli/log or $XDG_CACHE_HOME/dandi-cli/log macOS: ~/Library/Logs/dandi-cli Windows XP: C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\dandi\\dandi-cli\\Logs Windows Vista: C:\\Users\\<username>\\AppData\\Local\\dandi\\dandi-cli\\Logs Logs are named with a combination of the time at which the dandi command started running and the process ID of the command. Recent versions of dandi include all possible debugging information in the logs, but if you're using an older version, only log messages that were printed to the user when the command ran are recorded. As a result, in order to get complete debugging information, you may have to rerun the problematic command, this time increasing the logging level by passing -l DEBUG or --log-level DEBUG on the command line. Note that this option goes between the main dandi command and the name of the subcommand: # Right: dandi -l DEBUG upload # Wrong: dandi upload -l DEBUG In addition, many commands can be put into a developer-specific mode for showing raw progress information instead of fancy progress bars. For the delete , organize , upload , and validate commands, this can be done by setting the DANDI_DEVEL environment variable and passing --devel-debug to the command, like so: DANDI_DEVEL=1 dandi upload --devel-debug For the download command, the equivalent is the -f debug / --format debug option: dandi download -f debug More advanced users who are familiar with the Python debugger can instruct dandi to automatically open the debugger if any errors occur by supplying the --pdb option to the command. Like the -l / --log-level option, the --pdb option must be placed between dandi and the name of the subcommand.","title":"Acquiring Debugging Information"},{"location":"11_using_dandi_web/","text":"The DANDI Archive Web \u00b6 The DANDI Archive Web application allows you to browse and search across Dandisets . You can also create an account to register a new Dandiset , gain access to the Dandihub analysis platform , retrieve an API key to upload data to your Dandisets, and finally publish your Dandisets`. Exploring Archive without Registration \u00b6 Browse Dandisets \u00b6 When you go the DANDI Web application you can click on PUBLIC DANDISET to have access to all Dandisets currently available in the archive and you can sort them by name, identifier or date of modification. Search Dandisets \u00b6 In addition you can search across the Dandisets for specific contributor name or phrase, e.g. \"house mouse\", to get a subset of all Dandisets : When you click on one of the Dandisets you can see that the searching phrase could appear in the description, keywords, or in the assets summary. Dandisets metadata \u00b6 The landing page of each Dandiset contains important information including metadata provided by the owners such as contact information, description, license, access information and keywords, or simple statistics for Dandiset such as size of the Dandiset and number of files. If you scroll down, you will also find: - Assets Summary - Funding Information - Related Resources More detailed information you can find after choosing Metadata on the right side panel. If you have written access this is the place where you can edit the metadata, you can also download full metadata as a jason file. Using public Dandisets \u00b6 On the right hand panel, in addition to Metadata , you can find information how to cite the data and how to download the data: You can find information on how to install dandi-cli and how to use the software to download the data on your computer. The right side panel allows you also to access the list of folders and files. Using Archive to share data \u00b6 Register to Dandi Archive \u00b6 In order to use the Archive to share your own data, you have to create an account using your GitHub account: Once you register to the Archive, you can access your API key by clickin on your profile button: Creating a new Dandiset \u00b6 As a registered user you can create a new Dandiset and provide a few mandator fields - name and description: Once you create a new Dandiset , you can see it under the MY DANDISET section and you should add more metadata.","title":"The DANDI Archive Web"},{"location":"11_using_dandi_web/#the-dandi-archive-web","text":"The DANDI Archive Web application allows you to browse and search across Dandisets . You can also create an account to register a new Dandiset , gain access to the Dandihub analysis platform , retrieve an API key to upload data to your Dandisets, and finally publish your Dandisets`.","title":"The DANDI Archive Web"},{"location":"11_using_dandi_web/#exploring-archive-without-registration","text":"","title":"Exploring Archive without Registration"},{"location":"11_using_dandi_web/#browse-dandisets","text":"When you go the DANDI Web application you can click on PUBLIC DANDISET to have access to all Dandisets currently available in the archive and you can sort them by name, identifier or date of modification.","title":"Browse Dandisets"},{"location":"11_using_dandi_web/#search-dandisets","text":"In addition you can search across the Dandisets for specific contributor name or phrase, e.g. \"house mouse\", to get a subset of all Dandisets : When you click on one of the Dandisets you can see that the searching phrase could appear in the description, keywords, or in the assets summary.","title":"Search Dandisets"},{"location":"11_using_dandi_web/#dandisets-metadata","text":"The landing page of each Dandiset contains important information including metadata provided by the owners such as contact information, description, license, access information and keywords, or simple statistics for Dandiset such as size of the Dandiset and number of files. If you scroll down, you will also find: - Assets Summary - Funding Information - Related Resources More detailed information you can find after choosing Metadata on the right side panel. If you have written access this is the place where you can edit the metadata, you can also download full metadata as a jason file.","title":"Dandisets metadata"},{"location":"11_using_dandi_web/#using-public-dandisets","text":"On the right hand panel, in addition to Metadata , you can find information how to cite the data and how to download the data: You can find information on how to install dandi-cli and how to use the software to download the data on your computer. The right side panel allows you also to access the list of folders and files.","title":"Using public Dandisets"},{"location":"11_using_dandi_web/#using-archive-to-share-data","text":"","title":"Using Archive to share data"},{"location":"11_using_dandi_web/#register-to-dandi-archive","text":"In order to use the Archive to share your own data, you have to create an account using your GitHub account: Once you register to the Archive, you can access your API key by clickin on your profile button:","title":"Register to Dandi Archive"},{"location":"11_using_dandi_web/#creating-a-new-dandiset","text":"As a registered user you can create a new Dandiset and provide a few mandator fields - name and description: Once you create a new Dandiset , you can see it under the MY DANDISET section and you should add more metadata.","title":"Creating a new Dandiset"},{"location":"20_project_structure/","text":"Project structure \u00b6 The DANDI project is organized around several Github repositories. The main ones are the following. The DANDI helpdesk. This repository contains our community help platform. You can submit issues or questions for discussion . The DANDI archive. This repository contains the code for deploying the client-side Web application frontend based on the Vuejs framework. The DANDI Python client. This repository contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. The DANDI Jupyterhub. This repository contains the code for deploying a Jupyterhub instance to support interaction with the DANDI archive. The DANDI API. This repository provides the code for the DANDI API. The DANDI schema. This repostiory provides the details and some supporting code for the DANDI metadata schema. The DANDI schema Python library. This repostiory provides a Python library for updating the schema and for creating and validating DANDI objects. The DANDI handbook. This repository provides the contents of this website. The DANDI Website. This repository provides an overview of the DANDI project and the team members and collaborators.","title":"Project structure"},{"location":"20_project_structure/#project-structure","text":"The DANDI project is organized around several Github repositories. The main ones are the following. The DANDI helpdesk. This repository contains our community help platform. You can submit issues or questions for discussion . The DANDI archive. This repository contains the code for deploying the client-side Web application frontend based on the Vuejs framework. The DANDI Python client. This repository contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. The DANDI Jupyterhub. This repository contains the code for deploying a Jupyterhub instance to support interaction with the DANDI archive. The DANDI API. This repository provides the code for the DANDI API. The DANDI schema. This repostiory provides the details and some supporting code for the DANDI metadata schema. The DANDI schema Python library. This repostiory provides a Python library for updating the schema and for creating and validating DANDI objects. The DANDI handbook. This repository provides the contents of this website. The DANDI Website. This repository provides an overview of the DANDI project and the team members and collaborators.","title":"Project structure"},{"location":"30_data_standards/","text":"Data standards \u00b6 DANDI requires uploaded data to adhere to community data standards. These standards help data curators package all of the necessary metadata, and provide a uniform structure so that data can be more easily understood and reused by future users. DANDI also leverages these standards to provide features like data validation and automatic metadata extraction and search. DANDI currently supports two data standards: For cellular neurophysiology, such as electrophysiology and optical physiology, use Neurodata Without Borders (NWB) For neuroimaging data, such as MRI, use Brain Imaging Data Structure (BIDS) For micoscopy data from immunostaining, we are participating in the development of the BIDS extension for microscopy , and using the draft version for current Dandisets. To share data on DANDI, you will first need to convert your data to an appropriate standard. If you would like help determining which standard is most appropriate for your data, please do not hesitate to reach out using the dandi helpdesk and we would be happy to assist. Neurodata Without Borders (NWB) \u00b6 NWB is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data. The NWB team supports APIs in Python ( PyNWB ) and MATLAB ( MatNWB ), with tutorials for writing data broken down by experiment type. See the NWB Tutorials page for more details. Also see the NWB Conversion Tools user guide for converting data for automated conversions from several popular proprietary data formats. The best way to get help from the NWB community is through the NWB user Slack channel . Brain Imaging Data Format (BIDS) \u00b6 BIDS is a way to organize and describe neuroimaging and behavioral data. See the Getting Started page for instructions for how to convert your neuroimaging data to BIDS. For microscopy and associated MR data, please use the current Microscopy Draft BIDS Extension Proposal .","title":"Data standards"},{"location":"30_data_standards/#data-standards","text":"DANDI requires uploaded data to adhere to community data standards. These standards help data curators package all of the necessary metadata, and provide a uniform structure so that data can be more easily understood and reused by future users. DANDI also leverages these standards to provide features like data validation and automatic metadata extraction and search. DANDI currently supports two data standards: For cellular neurophysiology, such as electrophysiology and optical physiology, use Neurodata Without Borders (NWB) For neuroimaging data, such as MRI, use Brain Imaging Data Structure (BIDS) For micoscopy data from immunostaining, we are participating in the development of the BIDS extension for microscopy , and using the draft version for current Dandisets. To share data on DANDI, you will first need to convert your data to an appropriate standard. If you would like help determining which standard is most appropriate for your data, please do not hesitate to reach out using the dandi helpdesk and we would be happy to assist.","title":"Data standards"},{"location":"30_data_standards/#neurodata-without-borders-nwb","text":"NWB is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data. The NWB team supports APIs in Python ( PyNWB ) and MATLAB ( MatNWB ), with tutorials for writing data broken down by experiment type. See the NWB Tutorials page for more details. Also see the NWB Conversion Tools user guide for converting data for automated conversions from several popular proprietary data formats. The best way to get help from the NWB community is through the NWB user Slack channel .","title":"Neurodata Without Borders (NWB)"},{"location":"30_data_standards/#brain-imaging-data-format-bids","text":"BIDS is a way to organize and describe neuroimaging and behavioral data. See the Getting Started page for instructions for how to convert your neuroimaging data to BIDS. For microscopy and associated MR data, please use the current Microscopy Draft BIDS Extension Proposal .","title":"Brain Imaging Data Format (BIDS)"},{"location":"40_development/","text":"Developer Notes \u00b6 This page contains crucial information for anyone starting work on the DANDI project. Overview \u00b6 The DANDI archive dev environment comprises three major pieces of software: dandi-api , dandiarchive , dandi-cli , and dandischema . dandi-api is a Django application that serves the DANDI REST API. The Django application makes use of several services to provide essential function for the API, including Postgres (to hold administrative data about the web application itself), Celery (to run asynchronous compute tasks as needed to implement API semantics), and RabbitMQ (to act as a message broker between Celery and the rest of the application). The easiest way to run the API along with its services is through a Docker Compose setup, as detailed in the dandi-api README . dandiarchive is the web frontend application; it connects to dandi-api and provides a user interface to all of the DANDI functionality. dandiarchive is a standard web application built with yarn . See the dandiarchive README for instructions on how to build it locally. dandi-cli is a Python command line tool used to manage downloading and uploading of data with the archive. You may need to make use of this tool in developing new features for the frontend and backend, but there are other methods of faking data in the system to work with as well. You can install dandi-cli with a command like pip install dandi (then invoke dandi on the command line to run the tool), or build it locally following the instructions in the dandi-cli README . dandischema is a Python library for creating, maintaining, and validating the DANDI metadata models for dandisets and assets. You may need to make use of this tool when improving models, or migrating metadata. You can install dandischema with a command like pip install dandischema . When releases are published through dandischema, corresponding json-schemas are generated in the releases folder of the dandi schema repo . Important Things to Know \u00b6 This section gathers some important yet small bullet points of knowledge, useful to dive into development on the DANDI project. Technologies Used \u00b6 This section details some of the foundational technologies used in the DANDI codebases. Some basic understanding of these technologies is the bare minimum requirement for contributing meaningfully, but keep in mind that the DANDI team can help you get spun up as well. dandiarchive \u00b6 JavaScript/TypeScript. The DANDI archive code is a standard JavaScript web application, but we try to implement new functionality using TypeScript. Vue/VueX. The application's components are written in Vue, and global application state is managed through VueX. Vuetify. The components make heavy use of the Vuetify component library. For general help with dandiarchive , contact @waxlamp. dandi-api \u00b6 Python3. The backend code is written in Python 3. Django/drf/drf-yasg. The API infrastructure is implemented through a Django application. This means that application resources must be mapped to Django models, while Django views mediate API responses. The REST endpoints are implemented via Django Rest Framework (DRF), while DRF-YASG is used to generate Swagger documentation. For general help with dandi-api , contact @waxlamp. Deployment \u00b6 The DANDI project uses automated services to continuously deploy both the dandi-api backend and the dandiarchive frontend. Heroku manages backend deployment automatically from the master branch of the dandi-api repository. For this reason it is important that pull requests pass all CI tests before they are merged. Heroku configuration is in turn managed by Terraform code stored in the dandi-infrastructure repository. If you need access to the Heroku DANDI organization, talk to @satra. Netlify manages the frontend deployment process. Similarly to dandi-api , these deployments are based on the master branch of dandiarchive . The netlify.toml file controls Netlify settings. The @dandibot GitHub account is the \"owner\" of the Netlify account used for this purpose; in order to get access to that account, speak to @satra. Code Hosting \u00b6 All code repositories are hosted on GitHub. The easiest way to contribute is to gain push access to the repositories by talking to @waxlamp; this way, you can create pull requests based on branches within the origin repositories, which in turn allows for Netlify deploy previews and Heroku staging previews to be built. However, this is not strictly required. You can contribute using the standard fork-and-pull-request model, but under this workflow we will lose the benefit of those previews. Email Lists \u00b6 The project's email domain name services are managed via Terraform as AWS Route 53 entries. This allows the API server to send emails to users, etc. It also means we need a way to forward incoming emails to the proper mailing list--this is accomplished with a service called ImprovMX . The email addresses info@dandiarchive.org and help@dandiarchive.org are advertised to users as general email addresses to use to ask for information or help; both of them are forwarded to dandi@mit.edu, a mailing list containing the leaders and developers of the project. The forwarding is done by the ImprovMX service, and more such email addresses can be created as needed within that service. If you need the credentials for logging into ImprovMX, speak to Roni Choudhury ( roni.choudhury@kitware.com ). Miscellaneous Tips and Information \u00b6 Use email address to log into dev Django admin panel \u00b6 Once dandi-api is up and running, you can access the Django admin panel at http://localhost:8000/admin. The login page asks for a \"username\" but really it is expecting the email address associated with the username. One easy trick here is to supply the username again as the email address when you are setting up the superuser during initial setup. Refresh GitHub login to log into prod Django admin panel \u00b6 To log into the production Django admin panel, you must simply be logged into the DANDI Archive production instance using an admin account. However, at times the Django admin panel login seems to expire while the login to DANDI Archive proper is still live. In this case, simply log out of DANDI, log back in, and then go to the Django admin panel URL (e.g., https://api.dandiarchive.org/admin) and you should be logged back in there. Why do incoming emails to dandiarchive.org look crazy? \u00b6 When a user emails help@dandiarchive.org or info@dandiarchive.org, those messages are forwarded to dandi@mit.edu (see above ) so that the dev team sees them. However, these emails arrive with a long, spammy-looking From address with a Heroku DNS domain; this seems to be an artifact of how mit.edu processes emails, and does not occur in general for, e.g., messages sent from the API server to users.","title":"Notes"},{"location":"40_development/#developer-notes","text":"This page contains crucial information for anyone starting work on the DANDI project.","title":"Developer Notes"},{"location":"40_development/#overview","text":"The DANDI archive dev environment comprises three major pieces of software: dandi-api , dandiarchive , dandi-cli , and dandischema . dandi-api is a Django application that serves the DANDI REST API. The Django application makes use of several services to provide essential function for the API, including Postgres (to hold administrative data about the web application itself), Celery (to run asynchronous compute tasks as needed to implement API semantics), and RabbitMQ (to act as a message broker between Celery and the rest of the application). The easiest way to run the API along with its services is through a Docker Compose setup, as detailed in the dandi-api README . dandiarchive is the web frontend application; it connects to dandi-api and provides a user interface to all of the DANDI functionality. dandiarchive is a standard web application built with yarn . See the dandiarchive README for instructions on how to build it locally. dandi-cli is a Python command line tool used to manage downloading and uploading of data with the archive. You may need to make use of this tool in developing new features for the frontend and backend, but there are other methods of faking data in the system to work with as well. You can install dandi-cli with a command like pip install dandi (then invoke dandi on the command line to run the tool), or build it locally following the instructions in the dandi-cli README . dandischema is a Python library for creating, maintaining, and validating the DANDI metadata models for dandisets and assets. You may need to make use of this tool when improving models, or migrating metadata. You can install dandischema with a command like pip install dandischema . When releases are published through dandischema, corresponding json-schemas are generated in the releases folder of the dandi schema repo .","title":"Overview"},{"location":"40_development/#important-things-to-know","text":"This section gathers some important yet small bullet points of knowledge, useful to dive into development on the DANDI project.","title":"Important Things to Know"},{"location":"40_development/#technologies-used","text":"This section details some of the foundational technologies used in the DANDI codebases. Some basic understanding of these technologies is the bare minimum requirement for contributing meaningfully, but keep in mind that the DANDI team can help you get spun up as well.","title":"Technologies Used"},{"location":"40_development/#dandiarchive","text":"JavaScript/TypeScript. The DANDI archive code is a standard JavaScript web application, but we try to implement new functionality using TypeScript. Vue/VueX. The application's components are written in Vue, and global application state is managed through VueX. Vuetify. The components make heavy use of the Vuetify component library. For general help with dandiarchive , contact @waxlamp.","title":"dandiarchive"},{"location":"40_development/#dandi-api","text":"Python3. The backend code is written in Python 3. Django/drf/drf-yasg. The API infrastructure is implemented through a Django application. This means that application resources must be mapped to Django models, while Django views mediate API responses. The REST endpoints are implemented via Django Rest Framework (DRF), while DRF-YASG is used to generate Swagger documentation. For general help with dandi-api , contact @waxlamp.","title":"dandi-api"},{"location":"40_development/#deployment","text":"The DANDI project uses automated services to continuously deploy both the dandi-api backend and the dandiarchive frontend. Heroku manages backend deployment automatically from the master branch of the dandi-api repository. For this reason it is important that pull requests pass all CI tests before they are merged. Heroku configuration is in turn managed by Terraform code stored in the dandi-infrastructure repository. If you need access to the Heroku DANDI organization, talk to @satra. Netlify manages the frontend deployment process. Similarly to dandi-api , these deployments are based on the master branch of dandiarchive . The netlify.toml file controls Netlify settings. The @dandibot GitHub account is the \"owner\" of the Netlify account used for this purpose; in order to get access to that account, speak to @satra.","title":"Deployment"},{"location":"40_development/#code-hosting","text":"All code repositories are hosted on GitHub. The easiest way to contribute is to gain push access to the repositories by talking to @waxlamp; this way, you can create pull requests based on branches within the origin repositories, which in turn allows for Netlify deploy previews and Heroku staging previews to be built. However, this is not strictly required. You can contribute using the standard fork-and-pull-request model, but under this workflow we will lose the benefit of those previews.","title":"Code Hosting"},{"location":"40_development/#email-lists","text":"The project's email domain name services are managed via Terraform as AWS Route 53 entries. This allows the API server to send emails to users, etc. It also means we need a way to forward incoming emails to the proper mailing list--this is accomplished with a service called ImprovMX . The email addresses info@dandiarchive.org and help@dandiarchive.org are advertised to users as general email addresses to use to ask for information or help; both of them are forwarded to dandi@mit.edu, a mailing list containing the leaders and developers of the project. The forwarding is done by the ImprovMX service, and more such email addresses can be created as needed within that service. If you need the credentials for logging into ImprovMX, speak to Roni Choudhury ( roni.choudhury@kitware.com ).","title":"Email Lists"},{"location":"40_development/#miscellaneous-tips-and-information","text":"","title":"Miscellaneous Tips and Information"},{"location":"40_development/#use-email-address-to-log-into-dev-django-admin-panel","text":"Once dandi-api is up and running, you can access the Django admin panel at http://localhost:8000/admin. The login page asks for a \"username\" but really it is expecting the email address associated with the username. One easy trick here is to supply the username again as the email address when you are setting up the superuser during initial setup.","title":"Use email address to log into dev Django admin panel"},{"location":"40_development/#refresh-github-login-to-log-into-prod-django-admin-panel","text":"To log into the production Django admin panel, you must simply be logged into the DANDI Archive production instance using an admin account. However, at times the Django admin panel login seems to expire while the login to DANDI Archive proper is still live. In this case, simply log out of DANDI, log back in, and then go to the Django admin panel URL (e.g., https://api.dandiarchive.org/admin) and you should be logged back in there.","title":"Refresh GitHub login to log into prod Django admin panel"},{"location":"40_development/#why-do-incoming-emails-to-dandiarchiveorg-look-crazy","text":"When a user emails help@dandiarchive.org or info@dandiarchive.org, those messages are forwarded to dandi@mit.edu (see above ) so that the dev team sees them. However, these emails arrive with a long, spammy-looking From address with a Heroku DNS domain; this seems to be an artifact of how mit.edu processes emails, and does not occur in general for, e.g., messages sent from the API server to users.","title":"Why do incoming emails to dandiarchive.org look crazy?"},{"location":"about/policies/","text":"General Policies v1.0 \u00b6 Content \u00b6 Scope: Neurophysiology research. Raw and derived experimental data. Content must not violate privacy or copyright, or breach confidentiality or non-disclosure agreements for data collected from human subjects. Status of research data: Empirical (not simulated) data and associated metadata from any stage of the research study life-cycle is accepted. Simulated data is handled on a case-by-case basis, please contact the DANDI team Eligible users: Anyone working with the data in the scope of the archive may register as a user of DANDI. All users are allowed to deposit content for which they possess the appropriate rights and which falls within the scope of the archive. Ownership: By uploading content, no change of ownership is implied and no property rights are transferred to the DANDI team. All uploaded content remains the property of the parties prior to submission, and must be accompanied with a License allowing DANDI project data access, archival, and re-distribution (see License below). Data file formats: DANDI only accepts data using standardized formats such as Neurodata Without Borders , Brain Imaging Data Structure , Neuroimaging Data Model and other BRAIN Initiative standards. We are working with the community to improve these standards and to make DANDI archive FAIR. Volume and size limitations: There is a limit of 5TB per file and we currently accept any size of standardized datasets, as long as you can upload them over an HTTPS connection. However, if you plan to upload more than 10TB of data please get in touch with us. Data quality: All data are provided \u201cas-is\u201d, and the user shall hold DANDI and data providers supplying data to the DANDI Archive free and harmless in connection with the use of such data. Metadata types and sources: All metadata is stored internally in JSON-format according to a defined JSON schema. Metadata records violating the schema are not allowed. Language: Textual items must be in English. Latin names could be used in exceptional cases where appropriate. Licenses: Users must specify a license for each dataset chosen from the list of the DANDI archive approved licenses. Users allow for the DANDI archive to extract metadata records and make them available under permissive CC0 license. Access and Reuse \u00b6 Access to data objects: Files deposited to the archive are accessible to the public openly or accessible to collaborators for embargoed datasets. Access to metadata and data files is provided over standard protocols such as HTTPS. Use and re-use of data objects: Use and re-use is subject to the terms of the license under which the data objects were deposited. Metadata access and reuse: Metadata records, provided by the users or extracted from the assets, are licensed under CC0. All metadata is made publicly available and can be harvested. Embargo status: Users may deposit content under an embargo status and provide an anticipated end date for the embargo. The repository will restrict access to the data until the end of the embargo period; at which time, the content will become publically available automatically. The end of the embargo period is the earliest of the date provided by submitter, the first publication using the data, or the end of funding support for the collection and/or disemmination of the dataset. Restricted Access: Depositors of embargoed datasets have the ability to share access with other collaborators. These files will not be made publicly available till the end of the embargo period. Removal \u00b6 Revocation: Content not considered to fall under the scope of the repository can be removed and associated DOIs issued by DANDI revoked. Please inform DANDI team promptly, ideally no later than 24 hours from upload, about any suspected policy violation. Alternatively, content found to already have an external DOI will have the DANDI DOI invalidated and the record updated to indicate the original external DOI. User access may be revoked on violation of Terms of Use. Withdrawal: If the uploaded research object must later be withdrawn, the reason for the withdrawal will be indicated on a tombstone page, which will henceforth be served in its place. Withdrawal is considered an exceptional action, which normally should be requested and fully justified by the original uploader. In any other circumstance reasonable attempts will be made to contact the original uploader to obtain consent. The DOI and the URL of the original object are retained. User data on Dandihub: At present, user data on Dandihub is being removed periodically and Dandihub storage space should not be considered persistent. Longevity \u00b6 Versions: Datasets are versioned when published. Prior to publishing the state of a dataset may continue to evolve and the data or metadata are neither versioned, nor guaranteed to persist. Derivatives of data files may be generated, but original content is never modified. Replicas: All data files are stored on an AWS public bucket, with replicas housed at Dartmouth College. Data files are kept in multiple replicas at the moment, but this may change over time, and no recovery mechanisms for unversioned data should be assumed to be in place. Retention period: Versioned items will be retained for the lifetime of the repository. This is currently the lifetime of the NIH award, which currently expires in April 2024. Functional preservation: DANDI makes no promises of usability and understandability of deposited objects. File preservation: Data files and metadata are backed up nightly and replicated into multiple copies in different storage services. Fixity and authenticity: All data files are stored along with multiple checksums of the file content. Files are regularly checked against their checksums to assure that file content remains constant. Succession plans: In case of the repository shut down, best efforts will be made to integrate all content into suitable alternative institutional and/or other repositories overlapping in the scope of the DANDI archive. This policy document is derived from the Zenodo General Policies v1.0 .","title":"Policies"},{"location":"about/policies/#general-policies-v10","text":"","title":"General Policies v1.0"},{"location":"about/policies/#content","text":"Scope: Neurophysiology research. Raw and derived experimental data. Content must not violate privacy or copyright, or breach confidentiality or non-disclosure agreements for data collected from human subjects. Status of research data: Empirical (not simulated) data and associated metadata from any stage of the research study life-cycle is accepted. Simulated data is handled on a case-by-case basis, please contact the DANDI team Eligible users: Anyone working with the data in the scope of the archive may register as a user of DANDI. All users are allowed to deposit content for which they possess the appropriate rights and which falls within the scope of the archive. Ownership: By uploading content, no change of ownership is implied and no property rights are transferred to the DANDI team. All uploaded content remains the property of the parties prior to submission, and must be accompanied with a License allowing DANDI project data access, archival, and re-distribution (see License below). Data file formats: DANDI only accepts data using standardized formats such as Neurodata Without Borders , Brain Imaging Data Structure , Neuroimaging Data Model and other BRAIN Initiative standards. We are working with the community to improve these standards and to make DANDI archive FAIR. Volume and size limitations: There is a limit of 5TB per file and we currently accept any size of standardized datasets, as long as you can upload them over an HTTPS connection. However, if you plan to upload more than 10TB of data please get in touch with us. Data quality: All data are provided \u201cas-is\u201d, and the user shall hold DANDI and data providers supplying data to the DANDI Archive free and harmless in connection with the use of such data. Metadata types and sources: All metadata is stored internally in JSON-format according to a defined JSON schema. Metadata records violating the schema are not allowed. Language: Textual items must be in English. Latin names could be used in exceptional cases where appropriate. Licenses: Users must specify a license for each dataset chosen from the list of the DANDI archive approved licenses. Users allow for the DANDI archive to extract metadata records and make them available under permissive CC0 license.","title":"Content"},{"location":"about/policies/#access-and-reuse","text":"Access to data objects: Files deposited to the archive are accessible to the public openly or accessible to collaborators for embargoed datasets. Access to metadata and data files is provided over standard protocols such as HTTPS. Use and re-use of data objects: Use and re-use is subject to the terms of the license under which the data objects were deposited. Metadata access and reuse: Metadata records, provided by the users or extracted from the assets, are licensed under CC0. All metadata is made publicly available and can be harvested. Embargo status: Users may deposit content under an embargo status and provide an anticipated end date for the embargo. The repository will restrict access to the data until the end of the embargo period; at which time, the content will become publically available automatically. The end of the embargo period is the earliest of the date provided by submitter, the first publication using the data, or the end of funding support for the collection and/or disemmination of the dataset. Restricted Access: Depositors of embargoed datasets have the ability to share access with other collaborators. These files will not be made publicly available till the end of the embargo period.","title":"Access and Reuse"},{"location":"about/policies/#removal","text":"Revocation: Content not considered to fall under the scope of the repository can be removed and associated DOIs issued by DANDI revoked. Please inform DANDI team promptly, ideally no later than 24 hours from upload, about any suspected policy violation. Alternatively, content found to already have an external DOI will have the DANDI DOI invalidated and the record updated to indicate the original external DOI. User access may be revoked on violation of Terms of Use. Withdrawal: If the uploaded research object must later be withdrawn, the reason for the withdrawal will be indicated on a tombstone page, which will henceforth be served in its place. Withdrawal is considered an exceptional action, which normally should be requested and fully justified by the original uploader. In any other circumstance reasonable attempts will be made to contact the original uploader to obtain consent. The DOI and the URL of the original object are retained. User data on Dandihub: At present, user data on Dandihub is being removed periodically and Dandihub storage space should not be considered persistent.","title":"Removal"},{"location":"about/policies/#longevity","text":"Versions: Datasets are versioned when published. Prior to publishing the state of a dataset may continue to evolve and the data or metadata are neither versioned, nor guaranteed to persist. Derivatives of data files may be generated, but original content is never modified. Replicas: All data files are stored on an AWS public bucket, with replicas housed at Dartmouth College. Data files are kept in multiple replicas at the moment, but this may change over time, and no recovery mechanisms for unversioned data should be assumed to be in place. Retention period: Versioned items will be retained for the lifetime of the repository. This is currently the lifetime of the NIH award, which currently expires in April 2024. Functional preservation: DANDI makes no promises of usability and understandability of deposited objects. File preservation: Data files and metadata are backed up nightly and replicated into multiple copies in different storage services. Fixity and authenticity: All data files are stored along with multiple checksums of the file content. Files are regularly checked against their checksums to assure that file content remains constant. Succession plans: In case of the repository shut down, best efforts will be made to integrate all content into suitable alternative institutional and/or other repositories overlapping in the scope of the DANDI archive. This policy document is derived from the Zenodo General Policies v1.0 .","title":"Longevity"},{"location":"about/terms/","text":"Terms of Use v1.0 \u00b6 The DANDI data archive (\"DANDI\") is offered by the DANDI project as part of its mission to make available the results of its work. Use of DANDI, both the uploading and downloading of data, denotes agreement with the following terms: DANDI is an open dissemination research data repository for the preservation and making available of research, educational and informational content. Access to DANDI\u2019s content is open to all. Content may be uploaded free of charge by those without ready access to an organized data center. The uploader is exclusively responsible for the content that they upload to DANDI and shall indemnify and hold the DANDI team free and harmless in connection with their use of the service. The uploader shall ensure that their content is suitable for open dissemination, and that it complies with these terms and applicable laws, including, but not limited to, privacy, data protection and intellectual property rights [1]. In addition, where data that was originally sensitive personal data is being uploaded for open dissemination through DANDI, the uploader shall ensure that such data is either anonymized to an appropriate degree or fully consent cleared [2]. Access to DANDI, and all content, is provided on an \"as-is\" basis. Users of content (\"Users\") shall respect applicable license conditions. Download and use of content from DANDI does not transfer any intellectual property rights in the content to the User. Users are exclusively responsible for their use of content, and shall indemnify and hold the DANDI team free and harmless in connection with their download and/or use. Hosting and making content available through DANDI does not represent any approval or endorsement of such content by the DANDI team. The DANDI team reserves the right, without notice, at its sole discretion and without liability, (i) to alter, delete or block access to content that it deems to be inappropriate or insufficiently protected, and (ii) to restrict or remove User access where it considers that use of DANDI interferes with its operations or violates these Terms of Use or applicable laws. Unless specified otherwise, DANDI metadata may be freely reused under the CC0 waiver . These Terms of Use are subject to change by the DANDI team at any time and without notice, other than through posting the updated Terms of Use on the DANDI website. Uploaders considering DANDI for the storage of unanonymized or encrypted/unencrypted sensitive personal data are advised to use bespoke platforms rather than open dissemination services like DANDI for sharing their data. ** See further the user pages regarding uploading for information on anonymization of datasets that contain sensitive personal information. If you have any questions or comments with respect to DANDI, or if you are unsure whether your intended use is in line with these Terms of Use, or if you seek permission for a use that does not fall within these Terms of Use, please contact us . This Terms of Service document is derived from the Zenodo terms of service v1.2 .","title":"Terms"},{"location":"about/terms/#terms-of-use-v10","text":"The DANDI data archive (\"DANDI\") is offered by the DANDI project as part of its mission to make available the results of its work. Use of DANDI, both the uploading and downloading of data, denotes agreement with the following terms: DANDI is an open dissemination research data repository for the preservation and making available of research, educational and informational content. Access to DANDI\u2019s content is open to all. Content may be uploaded free of charge by those without ready access to an organized data center. The uploader is exclusively responsible for the content that they upload to DANDI and shall indemnify and hold the DANDI team free and harmless in connection with their use of the service. The uploader shall ensure that their content is suitable for open dissemination, and that it complies with these terms and applicable laws, including, but not limited to, privacy, data protection and intellectual property rights [1]. In addition, where data that was originally sensitive personal data is being uploaded for open dissemination through DANDI, the uploader shall ensure that such data is either anonymized to an appropriate degree or fully consent cleared [2]. Access to DANDI, and all content, is provided on an \"as-is\" basis. Users of content (\"Users\") shall respect applicable license conditions. Download and use of content from DANDI does not transfer any intellectual property rights in the content to the User. Users are exclusively responsible for their use of content, and shall indemnify and hold the DANDI team free and harmless in connection with their download and/or use. Hosting and making content available through DANDI does not represent any approval or endorsement of such content by the DANDI team. The DANDI team reserves the right, without notice, at its sole discretion and without liability, (i) to alter, delete or block access to content that it deems to be inappropriate or insufficiently protected, and (ii) to restrict or remove User access where it considers that use of DANDI interferes with its operations or violates these Terms of Use or applicable laws. Unless specified otherwise, DANDI metadata may be freely reused under the CC0 waiver . These Terms of Use are subject to change by the DANDI team at any time and without notice, other than through posting the updated Terms of Use on the DANDI website. Uploaders considering DANDI for the storage of unanonymized or encrypted/unencrypted sensitive personal data are advised to use bespoke platforms rather than open dissemination services like DANDI for sharing their data. ** See further the user pages regarding uploading for information on anonymization of datasets that contain sensitive personal information. If you have any questions or comments with respect to DANDI, or if you are unsure whether your intended use is in line with these Terms of Use, or if you seek permission for a use that does not fall within these Terms of Use, please contact us . This Terms of Service document is derived from the Zenodo terms of service v1.2 .","title":"Terms of Use v1.0"}]}